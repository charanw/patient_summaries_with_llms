{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNod3R70f6Sx"
      },
      "source": [
        "# Fine-tune models for replication and extension of \"A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SghW-lycgFUE"
      },
      "source": [
        "## Install depedencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9tLiZ9Ji4V9p",
        "outputId": "bc4b1c06-04e6-4ccd-faaf-76643b099435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.46.1 \\\n",
        "            accelerate==0.34.2 \\\n",
        "            datasets==3.0.0 \\\n",
        "            peft==0.11.1 \\\n",
        "            trl==0.9.4 \\\n",
        "            rouge_score \\\n",
        "            bert_score \\\n",
        "            sacremoses \\\n",
        "            sacrebleu \\\n",
        "            evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V56AI34JgH6A"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tvArpCSreLth"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "import wandb\n",
        "import json\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQtX_N69gKGX"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1-wcD58d4_O_"
      },
      "outputs": [],
      "source": [
        "# Set model and paths here\n",
        "device = \"cuda\"\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "train = \"/content/data/hallucinations_mimic_di_cleaned_improved.json\"\n",
        "val = \"/content/data/hallucinations_mimic_di_validation_cleaned_improved.json\"\n",
        "test = \"/content/data/hallucinations_mimic_di_validation_cleaned_improved.json\"\n",
        "adapter_path = \"/content/mistralai_Mistral-7B-Instruct-v0.3_cleaned_improved_ft\" # For evaluation\n",
        "predictions = \"\" # For evaluation\n",
        "training_data_used = \"cleaned_improved\"\n",
        "output = f\"./{model_name.replace('/','_')}_{training_data_used}_ft\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5hcff0ZgRsj"
      },
      "source": [
        "## Huggingface Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ma4R4jzI03t"
      },
      "outputs": [],
      "source": [
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWqflt21iikS"
      },
      "source": [
        "## Upload and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eE-e2qp4-mk"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"data/{filename}\")\n",
        "\n",
        "# Load data\n",
        "data = load_dataset(\"json\", data_files={\n",
        "    \"train\": train,\n",
        "    \"validation\": val,\n",
        "    \"test\": test\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfD92KVP0ps4"
      },
      "source": [
        "## Prompt formatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5I_uZqChJ8C"
      },
      "outputs": [],
      "source": [
        "# Prompts\n",
        "instruction = \"Summarize for the patient what happened during the hospital stay based on this doctor's note:\\n\"\n",
        "response = \"Summary for the patient:\\n\"\n",
        "\n",
        "# Prompt format\n",
        "def format_batch(batch):\n",
        "\n",
        "    outputs = []\n",
        "    for text, summary in zip(batch[\"text\"], batch[\"summary\"]):\n",
        "        outputs.append(\n",
        "            f\"{instruction}\"\n",
        "            f\"{text}\\n\\n\"\n",
        "            f\"{response}\"\n",
        "            f\"{summary}\"\n",
        "        )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Bwm9GH2h3y"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgF2LFvk2hpb"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj1F_q7pioE4"
      },
      "source": [
        "## Collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms1c9K1K1pIT"
      },
      "source": [
        "### For Llama 2/Mistral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5ZXZjd81o7k"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACAJZtcp1k8Z"
      },
      "source": [
        "### For Qwen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dosKDy__hLii"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForCompletionOnlyLM(\n",
        "    instruction_template=instruction,\n",
        "    response_template=response,\n",
        "    tokenizer=tokenizer\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsgH1yqSiuaw"
      },
      "source": [
        "## Set up model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVELcsXp2xrI"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbEC1SeK26qG"
      },
      "source": [
        "## Setup LoRA & trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3UOV-T-438J"
      },
      "outputs": [],
      "source": [
        "# LoRA setup\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # For Llama 2 replication/Mistral\n",
        "#     target_modules = [\n",
        "#     \"q_proj\",\n",
        "#     \"k_proj\",\n",
        "#     \"v_proj\",\n",
        "#     \"o_proj\",\n",
        "#     \"gate_proj\",\n",
        "#     \"up_proj\",\n",
        "#     \"down_proj\",\n",
        "# ], # For Qwen\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output,\n",
        "    max_seq_length=4096,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=20,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,\n",
        "    learning_rate=2e-5,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=data[\"train\"],\n",
        "    eval_dataset=data[\"validation\"],\n",
        "    formatting_func=format_batch,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4MDBVXOi1t-"
      },
      "source": [
        "## Run trainer and save model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KCQ4QMm8iz7_"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "\n",
        "trainer.model.save_pretrained(output)\n",
        "tokenizer.save_pretrained(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24NjBI9oUQz9"
      },
      "source": [
        "### Optionally zip for download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5q65zFAsTHf5"
      },
      "outputs": [],
      "source": [
        "!zip -r Qwen_Qwen2.5-7B-Instruct_cleaned_improved_ft.zip Qwen_Qwen2.5-7B-Instruct_cleaned_improved_ft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBjPAD3SDeo"
      },
      "source": [
        "# Helper functions\n",
        "\n",
        "Adapted from Hegselmann et al"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3ymWvhMBq-7"
      },
      "outputs": [],
      "source": [
        "# Use custom rouge function to obtain rouge 3/4 which are not available in huggingface\n",
        "def get_rouge_score(gold, pred):\n",
        "    rouge_scores = [\"rouge1\", \"rouge2\", \"rouge3\", \"rouge4\", \"rougeL\"]\n",
        "    scorer = rouge_scorer.RougeScorer(rouge_scores, use_stemmer=True)\n",
        "    scores = scorer.score(gold, pred)\n",
        "    return {k: scores[k].fmeasure * 100 for k in rouge_scores}\n",
        "\n",
        "\n",
        "def compute_custom_metrics(srcs, golds, preds, device):\n",
        "    scores = defaultdict(list)\n",
        "    bertscore = evaluate.load(\"bertscore\")\n",
        "    sari = evaluate.load(\"sari\")\n",
        "\n",
        "    # For rouge and length go over examples one by one and determine mean\n",
        "    for gold, pred in zip(golds, preds):\n",
        "        for k, v in get_rouge_score(gold, pred).items():\n",
        "            scores[k].append(v)\n",
        "        scores[\"words\"].append(len(pred.split(\" \")))\n",
        "    for k, v in scores.items():\n",
        "        scores[k] = np.mean(v)\n",
        "\n",
        "    # This is the default call using model_type=\"roberta-large\"\n",
        "    # This is the same as in the paper \"Generation of Patient After-Visit Summaries to Support Physicians\" (AVS_gen/eval_summarization.py) using the libary SummerTime\n",
        "    scores[\"bert_score\"] = (\n",
        "        np.mean(\n",
        "            (\n",
        "                bertscore.compute(\n",
        "                    predictions=preds, references=golds, lang=\"en\", device=device\n",
        "                )\n",
        "            )[\"f1\"]\n",
        "        )\n",
        "        * 100\n",
        "    )\n",
        "    # BERTScore authors recommend \"microsoft/deberta-large-mnli\" (https://github.com/Tiiiger/bert_score)\n",
        "    scores[\"bert_score_deberta-large\"] = (\n",
        "        np.mean(\n",
        "            (\n",
        "                bertscore.compute(\n",
        "                    predictions=preds,\n",
        "                    references=golds,\n",
        "                    device=device,\n",
        "                    model_type=\"microsoft/deberta-large-mnli\",\n",
        "                )\n",
        "            )[\"f1\"]\n",
        "        )\n",
        "        * 100\n",
        "    )\n",
        "    scores[\"sari\"] = sari.compute(\n",
        "        sources=srcs, predictions=preds, references=[[g] for g in golds]\n",
        "    )[\"sari\"]\n",
        "    # scores['sari'] = scores['sari'][0]\n",
        "    # Importing readability for dallc score not working: https://pypi.org/project/py-readability-metrics/\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def print_metrics_as_latex(metrics):\n",
        "    # Print latex table row\n",
        "    order = [\n",
        "        \"rouge1\",\n",
        "        \"rouge2\",\n",
        "        \"rouge3\",\n",
        "        \"rouge4\",\n",
        "        \"rougeL\",\n",
        "        \"bert_score\",\n",
        "        \"bert_score_deberta-large\",\n",
        "        \"sari\",\n",
        "        \"words\",\n",
        "    ]\n",
        "    print(\" & \".join([f\"${metrics[k]:.2f}$\" for k in order]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlQ6izPkSUBp"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efLntOeVggEM"
      },
      "source": [
        "### Optionally unzip\n",
        "\n",
        "If the model is already fine-tuned, upload it into Drive and unzip it for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Boj9tOmHcOiB"
      },
      "outputs": [],
      "source": [
        "!unzip -o \"/content/drive/MyDrive/models/model.zip\" -d \"/content/model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydhk6P1QSVqg"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"patient-summary-eval\",\n",
        "    name=f\"eval_{model_name}_{training_data_used}\",\n",
        ")\n",
        "\n",
        "print(\"Loading fine-tuned model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "eval_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "eval_model.eval().to(device)\n",
        "\n",
        "# Use validation split (authors used validation as test)\n",
        "eval_data = data[\"validation\"]\n",
        "\n",
        "# Generate prompt helper\n",
        "def generate_prompt(text):\n",
        "    instruction = \"Summarize for the patient what happened during the hospital stay based on this doctor's note:\\n\"\n",
        "    response = \"Summary for the patient:\\n\"\n",
        "    return f\"{instruction}{text}\\n\\n{response}\"\n",
        "\n",
        "# Prediction helper\n",
        "def predict_one(text):\n",
        "    prompt = generate_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = eval_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=350,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded[len(prompt):].strip()\n",
        "\n",
        "# Load predictions\n",
        "if predictions and Path(predictions).exists():\n",
        "    print(f\"Found existing predictions at {predictions}. Using existing predictions.\")\n",
        "    preds = []\n",
        "    with open(predictions, \"r\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            preds.append(obj[\"summary\"])\n",
        "\n",
        "# Generate predictions if they don't exist\n",
        "else:\n",
        "    print(\"Generating predictions...\")\n",
        "    preds = []\n",
        "    for ex in tqdm(eval_data, desc=\"Predicting\", ncols=100):\n",
        "        preds.append(predict_one(ex[\"text\"]))\n",
        "\n",
        "\n",
        "    # Save predictions\n",
        "    out_dir = Path(output)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    pred_file = out_dir / f\"{model_name.replace(\"/\", \"_\")}_predictions_eval.jsonl\"\n",
        "\n",
        "    with open(pred_file, \"w\") as f:\n",
        "        for p in preds:\n",
        "            obj = {\"summary\": p}\n",
        "            f.write(json.dumps(obj) + \"\\n\")\n",
        "\n",
        "    print(\"Predictions saved to predictions_eval.jsonl\")\n",
        "\n",
        "\n",
        "\n",
        "# Print examples\n",
        "print(\"\\n=== Example Outputs ===\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(\"NOTE:\", eval_data[i][\"text\"][:400], \"...\")\n",
        "    print(\"GOLD:\", eval_data[i][\"summary\"])\n",
        "    print(\"PRED:\", preds[i])\n",
        "\n",
        "# Compute metrics\n",
        "print(\"Computing metrics...\")\n",
        "metrics = compute_custom_metrics(\n",
        "    srcs=[ex[\"text\"] for ex in eval_data],\n",
        "    golds=[ex[\"summary\"] for ex in eval_data],\n",
        "    preds=preds,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Log metrics to wandb\n",
        "metrics_float = {k: float(v) for k, v in metrics.items()}\n",
        "wandb.log(metrics_float)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJr3afFfVTY_"
      },
      "outputs": [],
      "source": [
        "!zip -r wandb_runs.zip wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyqfCPWGemtE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}